import gradio as gr
import os
import json
from pathlib import Path
from audio_to_text import audio_models
from src.recommend_inspect_item.recommed_inspect_item import process_clinical_case
from src.image_analysis.image_analysis import analyze_images
from src.Analysis_test_results import analyze_test_results
import data_auto_anaylse_with_research_inspiration as data_analysis
from synthetic_data_pipeline import SyntheticDataPipeline
from doctor_to_patient_data import simulate_doctor_patient_dialogue
import time
import pandas as pd
import matplotlib.pyplot as plt
from io import BytesIO
import base64
from benchmark_test import run_benchmark_from_gradio
import datetime

# SFT training imports
import torch
from datetime import datetime
import requests
# from camel.datagen.cotdatagen import CoTDataGenerator
from camel.models import ModelFactory
from camel.types import ModelPlatformType, ModelType
from camel.configs import ChatGPTConfig
from camel.agents import ChatAgent
from camel.datahubs.huggingface import HuggingFaceDatasetManager
from camel.datahubs.models import Record
# from unsloth import FastLanguageModel, is_bfloat16_supported
# from trl import SFTTrainer
from transformers import TrainingArguments
from datasets import load_dataset


# åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºå­˜å‚¨ä¸­é—´æ–‡ä»¶
os.makedirs("temp", exist_ok=True)
os.makedirs("temp/sft_data", exist_ok=True)
os.makedirs("temp/trained_models", exist_ok=True)
os.makedirs("export", exist_ok=True)

# å®šä¹‰åŒ»ç–—é£æ ¼ä¸»é¢˜é¢œè‰²å’ŒCSS
MEDICAL_THEME = gr.themes.Soft(
    primary_hue=gr.themes.Color(
        c50="#E3F2FD",
        c100="#BBDEFB",
        c200="#90CAF9",
        c300="#64B5F6",
        c400="#42A5F5",
        c500="#2196F3",  # åŒ»ç–—è“è‰²
        c600="#1E88E5",
        c700="#1976D2", 
        c800="#1565C0",
        c900="#0D47A1",
        c950="#0A2F6F",
    ),
    secondary_hue=gr.themes.Color(
        c50="#E8F5E9",
        c100="#C8E6C9",
        c200="#A5D6A7",
        c300="#81C784",
        c400="#66BB6A",
        c500="#4CAF50",  # åŒ»ç–—ç»¿è‰²
        c600="#43A047",
        c700="#388E3C",
        c800="#2E7D32",
        c900="#1B5E20",
        c950="#0F3E14",
    ),
    neutral_hue="slate",
    radius_size=gr.themes.sizes.radius_md,
    text_size=gr.themes.sizes.text_md,
).set(
    body_background_fill="#F8FAFC",
    body_background_fill_dark="#1E293B",
    body_text_color="#334155",
    body_text_color_dark="#E2E8F0",
    button_primary_background_fill="#2196F3",
    button_primary_background_fill_hover="#1976D2",
    button_primary_text_color="white",
    button_primary_border_color="#2196F3",
    button_secondary_background_fill="white",
    button_secondary_text_color="#2196F3",
    button_secondary_border_color="#2196F3",
    button_cancel_background_fill="#EF4444",
    button_cancel_background_fill_hover="#DC2626",
    button_cancel_text_color="white",
    button_cancel_border_color="#EF4444",
    block_background_fill="white",
    block_background_fill_dark="#334155",
    block_border_color="#E2E8F0",
    block_border_color_dark="#475569",
    block_radius="0.5rem",
    block_shadow="0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -2px rgba(0, 0, 0, 0.1)",
    block_title_text_color="#0F172A",
    block_title_text_color_dark="#F1F5F9",
    block_title_text_weight="600",
    block_label_text_color="#64748B",
    block_label_text_color_dark="#CBD5E1",
    block_label_text_weight="500",
    input_background_fill="white",
    input_background_fill_dark="#1E293B",
    input_border_color="#CBD5E1",
    input_border_color_dark="#475569",
    input_border_width="1px",
    input_background_fill_focus="white",
    input_background_fill_focus_dark="#1E293B",
    input_border_color_focus="#2196F3",
    input_border_color_focus_dark="#2196F3",
)

# è‡ªå®šä¹‰åŒ»ç–—é£æ ¼CSSæ ·å¼
CUSTOM_CSS = """
.gradio-container {
    font-family: "Inter", "Segoe UI", system-ui, -apple-system, sans-serif !important;
}

h1, h2, h3, h4 {
    font-family: "Inter", "Segoe UI", system-ui, -apple-system, sans-serif !important;
    font-weight: 700 !important;
    color: #0F172A !important;
}

h1 {
    font-size: 1.8rem !important;
    border-bottom: 2px solid #2196F3 !important;
    padding-bottom: 0.5rem !important;
    margin-bottom: 1.5rem !important;
    position: relative !important;
}

h1::before {
    content: "ğŸ¥" !important;
    margin-right: 0.5rem !important;
}

.tab-nav {
    border-bottom: 1px solid #E2E8F0 !important;
    margin-bottom: 20px !important;
    background-color: rgba(33, 150, 243, 0.05) !important;
    border-radius: 8px 8px 0 0 !important;
}

.tab-nav button {
    font-weight: 600 !important;
    border-radius: 8px 8px 0 0 !important;
    padding: 10px 16px !important;
    margin: 0 2px !important;
    color: #64748B !important;
    border: none !important;
}

.tab-nav button:hover {
    background-color: rgba(33, 150, 243, 0.1) !important;
    color: #1E88E5 !important;
}

.tab-nav button.selected {
    border-bottom: 3px solid #2196F3 !important;
    color: #2196F3 !important;
    background-color: rgba(33, 150, 243, 0.08) !important;
}

.block {
    border: 1px solid #E2E8F0 !important;
    border-radius: 8px !important;
    box-shadow: 0 4px 6px -1px rgba(0, 0, 0, 0.1), 0 2px 4px -2px rgba(0, 0, 0, 0.05) !important;
    background-color: white !important;
    padding: 1rem !important;
    margin-bottom: 1rem !important;
    transition: transform 0.2s, box-shadow 0.2s !important;
}

.block:hover {
    box-shadow: 0 10px 15px -3px rgba(0, 0, 0, 0.1), 0 4px 6px -4px rgba(0, 0, 0, 0.05) !important;
}

.block-label {
    font-weight: 600 !important;
    color: #334155 !important;
    font-size: 0.9rem !important;
    margin-bottom: 0.3rem !important;
}

.primary-btn {
    text-transform: capitalize !important;
    font-size: 14px !important;
    font-weight: 600 !important;
    letter-spacing: 0.3px !important;
    border-radius: 6px !important;
    transition: all 0.3s !important;
    padding: 8px 16px !important;
}

.primary-btn:hover {
    transform: translateY(-1px) !important;
    box-shadow: 0 4px 8px rgba(33, 150, 243, 0.3) !important;
}

button.secondary {
    border: 1px solid #2196F3 !important;
    color: #2196F3 !important;
    background-color: white !important;
    transition: all 0.3s !important;
}

button.secondary:hover {
    background-color: rgba(33, 150, 243, 0.05) !important;
}

.gr-input, .gr-textarea {
    border: 1px solid #CBD5E1 !important;
    border-radius: 6px !important;
    padding: 8px 12px !important;
    transition: border-color 0.3s, box-shadow 0.3s !important;
}

.gr-input:focus, .gr-textarea:focus {
    border-color: #2196F3 !important;
    box-shadow: 0 0 0 3px rgba(33, 150, 243, 0.2) !important;
    outline: none !important;
}

.gr-panel {
    border-radius: 8px !important;
    overflow: hidden !important;
}

.gr-accordion {
    border: 1px solid #E2E8F0 !important;
    border-radius: 8px !important;
    margin-bottom: 1rem !important;
}

.gr-accordion-header {
    background-color: rgba(33, 150, 243, 0.05) !important;
    padding: 12px 16px !important;
    font-weight: 600 !important;
    color: #334155 !important;
}

.footer {
    text-align: center !important;
    margin-top: 2rem !important;
    padding: 1rem !important;
    color: #64748B !important;
    font-size: 0.8rem !important;
    border-top: 1px solid #E2E8F0 !important;
}

/* æ–°å¢åŒ»ç–—é£æ ¼å…ƒç´  */
.medical-icon {
    color: #2196F3 !important;
    margin-right: 6px !important;
}

.medical-alert {
    background-color: #FFF4E5 !important;
    border-left: 4px solid #FF9800 !important;
    padding: 12px 16px !important;
    margin-bottom: 1rem !important;
    border-radius: 4px !important;
    color: #7B341E !important;
}

.medical-success {
    background-color: #E8F5E9 !important;
    border-left: 4px solid #4CAF50 !important;
    padding: 12px 16px !important;
    margin-bottom: 1rem !important;
    border-radius: 4px !important;
    color: #1B5E20 !important;
}

/* ä¸ºåŒ»ç–—æ•°æ®å±•ç¤ºä¼˜åŒ–æ ·å¼ */
table {
    border-collapse: collapse !important;
    width: 100% !important;
    border-radius: 8px !important;
    overflow: hidden !important;
}

th {
    background-color: #2196F3 !important;
    color: white !important;
    font-weight: 600 !important;
    text-align: left !important;
    padding: 12px !important;
}

td {
    padding: 10px 12px !important;
    border-bottom: 1px solid #E2E8F0 !important;
}

tr:nth-child(even) {
    background-color: #F8FAFC !important;
}

tr:hover {
    background-color: #E3F2FD !important;
}
"""

# 1. è¯­éŸ³è½¬æ–‡æœ¬é¡µé¢
def process_audio(audio_path):
    try:
        # è½¬æ¢éŸ³é¢‘ä¸ºæ–‡æœ¬
        converted_text = audio_models.speech_to_text(audio_file_path=audio_path)
        # ä¿å­˜è½¬æ¢åçš„æ–‡æœ¬åˆ°ä¸´æ—¶æ–‡ä»¶
        with open("temp/converted_text.txt", "w", encoding="utf-8") as f:
            f.write(converted_text)
        return converted_text
    except Exception as e:
        return f"å¤„ç†å¤±è´¥: {str(e)}"

def text_to_audio(input_text, output_path="temp/output_audio.mp3"):
    try:
        # å°†æ–‡æœ¬è½¬æ¢ä¸ºè¯­éŸ³
        audio_models.text_to_speech(input=input_text, storage_path=output_path)
        return output_path, "è¯­éŸ³ç”ŸæˆæˆåŠŸ"
    except Exception as e:
        return None, f"å¤„ç†å¤±è´¥: {str(e)}"

# 2. æ£€éªŒé¡¹ç›®æ¨èé¡µé¢
def recommend_inspection_items(conversation_text):
    try:
        # è°ƒç”¨é¡¹ç›®æ¨èæ¨¡å—
        result = process_clinical_case(conversation_text)
        
        # ä¿å­˜æ¨èç»“æœåˆ°ä¸´æ—¶æ–‡ä»¶
        with open("temp/recommendation_result.txt", "w", encoding="utf-8") as f:
            f.write(result)
        
        # åˆ›å»ºé—®ç­”æ•°æ®
        qa_data = {conversation_text: result}
        
        # ä¿å­˜é—®ç­”æ•°æ®åˆ°JSONæ–‡ä»¶
        save_qa_data_to_json(qa_data, "temp/recommendation_qa_data.json")
        
        return result
    except Exception as e:
        return f"å¤„ç†å¤±è´¥: {str(e)}"

def load_converted_text():
    try:
        with open("temp/converted_text.txt", "r", encoding="utf-8") as f:
            return f.read()
    except Exception:
        return ""

# 3. æ£€éªŒç»“æœå›¾åƒåˆ†æé¡µé¢
def analyze_test_images(image_paths, other_patient_info):
    try:
        # åˆ†æä¸Šä¼ çš„å›¾åƒ
        if not image_paths:
            return "è¯·ä¸Šä¼ è‡³å°‘ä¸€å¼ å›¾åƒ"
        
        image_analysis_result = analyze_images(image_paths)
        
        # å‡†å¤‡å®Œæ•´çš„æ‚£è€…ä¿¡æ¯
        full_patient_info = f"å›¾ç‰‡åˆ†æç»“æœï¼š\n{image_analysis_result}\n\nå…¶ä»–æ‚£è€…ä¿¡æ¯ï¼š\n{other_patient_info}"
        
        # ä¿å­˜åˆ†æç»“æœå’Œæ‚£è€…ä¿¡æ¯åˆ°ä¸´æ—¶æ–‡ä»¶
        with open("temp/image_analysis_result.txt", "w", encoding="utf-8") as f:
            f.write(full_patient_info)
            
        return "å›¾åƒåˆ†æå®Œæˆï¼Œç»“æœå·²ä¿å­˜"
    except Exception as e:
        return f"å¤„ç†å¤±è´¥: {str(e)}"

# 4. æ£€éªŒæŠ¥å‘Šç”Ÿæˆé¡µé¢
def generate_test_report():
    try:
        # è¯»å–å›¾åƒåˆ†æç»“æœå’Œæ‚£è€…ä¿¡æ¯
        with open("temp/image_analysis_result.txt", "r", encoding="utf-8") as f:
            patient_info = f.read()
        
        # è°ƒç”¨æŠ¥å‘Šç”Ÿæˆæ¨¡å—
        report = analyze_test_results(patient_info)
        
        # ä¿å­˜ç”Ÿæˆçš„æŠ¥å‘Š
        with open("temp/test_report.txt", "w", encoding="utf-8") as f:
            f.write(report)
        
        # åˆ›å»ºé—®ç­”æ•°æ®
        qa_data = {patient_info: report}
        
        # ä¿å­˜é—®ç­”æ•°æ®åˆ°JSONæ–‡ä»¶
        save_qa_data_to_json(qa_data, "temp/test_report_qa_data.json")
        
        return report
    except Exception as e:
        return f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"

# 5. æ£€éªŒæ•°æ®åˆ†æä¸ç§‘ç ”çµæ„Ÿé¡µé¢
def analyze_csv_data(csv_file_path, analysis_requirements):
    try:
        if csv_file_path:
            # ä½¿ç”¨ç”¨æˆ·ä¸Šä¼ çš„CSVæ–‡ä»¶
            result = data_analysis.process_lab_data_analysis(
                user_info=analysis_requirements,
                input_csv_path=csv_file_path
            )
        else:
            
            raise Exception("è¯·ä¸Šä¼ CSVæ–‡ä»¶")
        
        # ä¿å­˜åˆ†æç»“æœ
        with open("temp/data_analysis_result.txt", "w", encoding="utf-8") as f:
            f.write(result)
            
        return result
    except Exception as e:
        return f"æ•°æ®åˆ†æå¤±è´¥: {str(e)}"

# 6. åˆæˆæ•°æ®ç”Ÿæˆé¡µé¢
def generate_synthetic_data(main_topic, total_examples, num_subtopics, temperature):
    try:
        # åˆ›å»ºåˆæˆæ•°æ®ç”Ÿæˆå™¨
        pipeline = SyntheticDataPipeline(temperature=temperature)
        
        # ç”Ÿæˆæ•°æ®
        data = pipeline.generate_synthetic_data(
            main_topic=main_topic,
            total_examples=total_examples,
            num_subtopics=num_subtopics
        )
        
        # ä¿å­˜ç”Ÿæˆçš„æ•°æ®
        output_file = "temp/synthetic_data.json"
        pipeline.save_to_file(data, output_file)
        
        # è¯»å–å¹¶è¿”å›ç”Ÿæˆçš„æ•°æ®
        with open(output_file, "r", encoding="utf-8") as f:
            result = f.read()
            
        return result
    except Exception as e:
        return f"æ•°æ®ç”Ÿæˆå¤±è´¥: {str(e)}"

# 7. ç—…å†ç”ŸæˆåŒ»æ‚£å¯¹è¯é¡µé¢
def generate_doctor_patient_dialogue(patient_history):
    try:
        # ç”ŸæˆåŒ»æ‚£å¯¹è¯
        dialogue = simulate_doctor_patient_dialogue(patient_history)
        
        # ä¿å­˜ç”Ÿæˆçš„å¯¹è¯
        with open("temp/simulated_dialogue.txt", "w", encoding="utf-8") as f:
            f.write(dialogue)
            
        return dialogue
    except Exception as e:
        return f"å¯¹è¯ç”Ÿæˆå¤±è´¥: {str(e)}"

# åˆ›å»ºBenchmarkæµ‹è¯•ç›¸å…³å‡½æ•°
def run_benchmark(modules_to_test, test_samples):
    results = {
        "æ¨¡å—": [],
        "å¤„ç†æ—¶é—´(ç§’)": [],
        "å†…å­˜ä½¿ç”¨(MB)": [],
        "æˆåŠŸç‡(%)": []
    }
    
    for module_name, module_func, test_data in modules_to_test:
        print(f"æµ‹è¯•æ¨¡å—: {module_name}")
        total_time = 0
        success_count = 0
        memory_usage = 0
        
        for i, sample in enumerate(test_data[:test_samples]):
            start_time = time.time()
            try:
                # è¿è¡Œæ¨¡å—å‡½æ•°
                module_func(sample)
                success_count += 1
                # ç®€å•æ¨¡æ‹Ÿå†…å­˜ä½¿ç”¨ (å®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨æ›´å‡†ç¡®çš„æµ‹é‡æ–¹æ³•)
                memory_usage += len(str(sample)) * 0.001
            except Exception as e:
                print(f"æ ·æœ¬ {i+1} å¤±è´¥: {str(e)}")
            finally:
                end_time = time.time()
                total_time += (end_time - start_time)
        
        avg_time = total_time / test_samples if test_samples > 0 else 0
        success_rate = (success_count / test_samples) * 100 if test_samples > 0 else 0
        avg_memory = memory_usage / test_samples if test_samples > 0 else 0
        
        results["æ¨¡å—"].append(module_name)
        results["å¤„ç†æ—¶é—´(ç§’)"].append(round(avg_time, 3))
        results["å†…å­˜ä½¿ç”¨(MB)"].append(round(avg_memory, 2))
        results["æˆåŠŸç‡(%)"].append(round(success_rate, 1))
    
    return pd.DataFrame(results)

def plot_benchmark_results(df):
    # åˆ›å»ºæ€§èƒ½å›¾è¡¨
    fig, ax = plt.subplots(1, 3, figsize=(18, 6))
    
    # å¤„ç†æ—¶é—´å›¾è¡¨
    df.plot(x="æ¨¡å—", y="å¤„ç†æ—¶é—´(ç§’)", kind="bar", ax=ax[0], color="blue", legend=False)
    ax[0].set_title("å¹³å‡å¤„ç†æ—¶é—´ (ç§’)")
    ax[0].set_ylabel("ç§’")
    
    # å†…å­˜ä½¿ç”¨å›¾è¡¨
    df.plot(x="æ¨¡å—", y="å†…å­˜ä½¿ç”¨(MB)", kind="bar", ax=ax[1], color="green", legend=False)
    ax[1].set_title("å¹³å‡å†…å­˜ä½¿ç”¨ (MB)")
    ax[1].set_ylabel("MB")
    
    # æˆåŠŸç‡å›¾è¡¨
    df.plot(x="æ¨¡å—", y="æˆåŠŸç‡(%)", kind="bar", ax=ax[2], color="orange", legend=False)
    ax[2].set_title("æˆåŠŸç‡ (%)")
    ax[2].set_ylabel("%")
    
    plt.tight_layout()
    
    # å°†å›¾è¡¨è½¬æ¢ä¸ºbase64ç¼–ç çš„å›¾åƒ
    buf = BytesIO()
    plt.savefig(buf, format="png")
    buf.seek(0)
    img_str = base64.b64encode(buf.read()).decode("utf-8")
    plt.close()
    
    return df, f"data:image/png;base64,{img_str}"

def get_test_samples():
    # è¿”å›æµ‹è¯•æ ·æœ¬
    audio_samples = ["temp/output_audio.mp3"] * 5
    
    text_samples = [
        "æ‚£è€…ç”·æ€§ï¼Œ45å²ï¼Œä¸»è¯‰èƒ¸é—·æ°”çŸ­3ä¸ªæœˆï¼ŒåŠ é‡1å‘¨ã€‚æŸ¥ä½“ï¼šBP 160/95mmHgï¼Œå¿ƒç‡88æ¬¡/åˆ†ã€‚",
        "æ‚£è€…å¥³æ€§ï¼Œ35å²ï¼Œä¸»è¯‰å…³èŠ‚ç–¼ç—›ä¼´æ™¨åƒµ2å¹´ï¼ŒåŠ é‡1ä¸ªæœˆã€‚æŸ¥ä½“ï¼šåŒæ‰‹è¿‘ç«¯æŒ‡é—´å…³èŠ‚è‚¿èƒ€ã€‚",
        "æ‚£è€…ç”·æ€§ï¼Œ60å²ï¼Œä¸»è¯‰å°¿é¢‘å°¿æ€¥å°¿ç—›5å¤©ã€‚æŸ¥ä½“ï¼šä¸‹è…¹éƒ¨å‹ç—›ï¼Œè‚¾åŒºå©å‡»ç—›é˜´æ€§ã€‚",
        "æ‚£è€…å¥³æ€§ï¼Œ28å²ï¼Œä¸»è¯‰å‘çƒ­ã€å’³å—½ã€å’³ç—°3å¤©ã€‚æŸ¥ä½“ï¼šä½“æ¸©38.5â„ƒï¼ŒåŒè‚ºå¯é—»åŠæ¹¿å•°éŸ³ã€‚",
        "æ‚£è€…ç”·æ€§ï¼Œ50å²ï¼Œä¸»è¯‰ä¸Šè…¹ç—›ã€æ¶å¿ƒã€å‘•å2å¤©ã€‚æŸ¥ä½“ï¼šä¸Šè…¹éƒ¨å‹ç—›ï¼ŒMurphyå¾é˜³æ€§ã€‚"
    ]
    
    image_samples = [["i01.png", "i03.png"]] * 5
    
    return {
        "audio": audio_samples,
        "text": text_samples,
        "image": image_samples,
        "patient_histories": text_samples
    }

def run_system_benchmark(components_to_test, num_samples):
    test_samples = get_test_samples()
    modules_to_test = []
    
    # æ ¹æ®é€‰æ‹©çš„ç»„ä»¶å‡†å¤‡æµ‹è¯•æ¨¡å—
    if "è¯­éŸ³è½¬æ–‡æœ¬" in components_to_test:
        modules_to_test.append(("è¯­éŸ³è½¬æ–‡æœ¬", lambda x: process_audio(x), test_samples["audio"]))
    
    if "æ£€éªŒé¡¹ç›®æ¨è" in components_to_test:
        modules_to_test.append(("æ£€éªŒé¡¹ç›®æ¨è", lambda x: recommend_inspection_items(x), test_samples["text"]))
    
    if "å›¾åƒåˆ†æ" in components_to_test:
        modules_to_test.append(("å›¾åƒåˆ†æ", lambda x: analyze_test_images(x, "æµ‹è¯•æ‚£è€…ä¿¡æ¯"), test_samples["image"]))
    
    if "åŒ»æ‚£å¯¹è¯ç”Ÿæˆ" in components_to_test:
        modules_to_test.append(("åŒ»æ‚£å¯¹è¯ç”Ÿæˆ", lambda x: generate_doctor_patient_dialogue(x), test_samples["patient_histories"]))

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    results_df = run_benchmark(modules_to_test, num_samples)
    
    # ç”Ÿæˆç»“æœå›¾è¡¨
    df, img_data = plot_benchmark_results(results_df)
    
    # å°†ç»“æœä¿å­˜åˆ°æ–‡ä»¶
    results_df.to_csv("temp/benchmark_results.csv", index=False)
    
    return results_df, img_data

# COTæ•°æ®ç”Ÿæˆå’ŒSFTè®­ç»ƒç›¸å…³å‡½æ•°
def create_chat_agent(api_key, model_choice, system_message):
    """åˆ›å»ºç”¨äºç”ŸæˆCOTæ•°æ®çš„ChatAgent"""
    try:
        os.environ["OPENAI_API_KEY"] = api_key
        
        # å®šä¹‰ä¸åŒçš„æ¨¡å‹ç±»å‹
        model_types = {
           "gpt-4.1": ModelType.GPT_4_1,
            "gpt-4o-mini": ModelType.GPT_4_1_MINI,
            "gpt-4o": ModelType.GPT_4O,
        }
        
        # åˆ›å»ºæ¨¡å‹
        model = ModelFactory.create(
            model_platform=ModelPlatformType.OPENAI,
            model_type=model_types.get(model_choice, ModelType.GPT_4_1),
            model_config_dict=ChatGPTConfig().as_dict(),
        )
        
        # åˆ›å»ºChatAgent
        chat_agent = ChatAgent(
            system_message=system_message,
            model=model,
            message_window_size=10,
        )
        
        return chat_agent, "æˆåŠŸåˆ›å»ºChatAgent"
    except Exception as e:
        return None, f"åˆ›å»ºChatAgentå¤±è´¥: {str(e)}"

def generate_cot_data(chat_agent, input_qa_data, output_file="temp/sft_data/generated_answers.json"):
    """ç”Ÿæˆå¹¶ä¿å­˜CoTæ€ç»´é“¾æ•°æ®"""
    try:
        if not chat_agent:
            return "è¯·å…ˆåˆ›å»ºChatAgent"
        
        # è§£æè¾“å…¥çš„QAæ•°æ®
        qa_data = json.loads(input_qa_data) if isinstance(input_qa_data, str) else input_qa_data
        
        # åˆ›å»ºCoTDataGeneratorå®ä¾‹
        cot_generator = CoTDataGenerator(chat_agent, golden_answers=qa_data)
        
        # è®°å½•ç”Ÿæˆçš„ç­”æ¡ˆ
        generated_answers = {}
        generation_results = []
        
        # ä¸ºæ¯ä¸ªé—®é¢˜ç”Ÿæˆç­”æ¡ˆ
        for i, (question, golden_answer) in enumerate(qa_data.items(), 1):
            generation_results.append(f"é—®é¢˜ {i}: {question}")
            
            # è·å–AIçš„æ€è€ƒè¿‡ç¨‹å’Œç­”æ¡ˆ
            answer = cot_generator.get_answer(question)
            generated_answers[question] = answer
            generation_results.append(f"AIçš„æ€è€ƒè¿‡ç¨‹å’Œç­”æ¡ˆ:\n{answer}")
            
            # éªŒè¯ç­”æ¡ˆ
            is_correct = cot_generator.verify_answer(question, answer)
            generation_results.append(f"ç­”æ¡ˆéªŒè¯ç»“æœ: {'æ­£ç¡®' if is_correct else 'ä¸æ­£ç¡®'}")
            generation_results.append("-" * 50)
        
        # å¯¼å‡ºç”Ÿæˆçš„ç­”æ¡ˆ
        output_data = {
            'timestamp': datetime.now().isoformat(),
            'qa_pairs': generated_answers
        }
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(output_data, f, ensure_ascii=False, indent=2)
        
        # è½¬æ¢ä¸ºAlpacaè®­ç»ƒæ•°æ®æ ¼å¼
        alpaca_data = []
        for question, answer in generated_answers.items():
            alpaca_data.append({
                "instruction": question,
                "input": "",
                "output": answer
            })
        
        alpaca_file = f"temp/sft_data/alpaca_format_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(alpaca_file, 'w', encoding='utf-8') as f:
            json.dump(alpaca_data, f, ensure_ascii=False, indent=2)
        
        return "\n".join(generation_results), alpaca_file
    except Exception as e:
        return f"ç”ŸæˆCoTæ•°æ®å¤±è´¥: {str(e)}", None

def upload_dataset_to_huggingface(alpaca_file, hf_token, username, dataset_name=None):
    """å°†ç”Ÿæˆçš„æ•°æ®é›†ä¸Šä¼ åˆ°HuggingFace"""
    try:
        os.environ["HUGGING_FACE_TOKEN"] = hf_token
        
        # è¯»å–è½¬æ¢åçš„æ•°æ®
        with open(alpaca_file, 'r', encoding='utf-8') as f:
            transformed_data = json.load(f)
        
        # åˆå§‹åŒ–HuggingFaceDatasetManager
        manager = HuggingFaceDatasetManager()
        
        # ç”Ÿæˆæˆ–éªŒè¯æ•°æ®é›†åç§°
        if dataset_name is None:
            dataset_name = f"{username}/medical-qa-dataset-{datetime.now().strftime('%Y%m%d')}"
        else:
            dataset_name = f"{username}/{dataset_name}"
        
        # åˆ›å»ºæ•°æ®é›†
        print(f"åˆ›å»ºæ•°æ®é›†: {dataset_name}")
        dataset_url = manager.create_dataset(name=dataset_name)
        
        # åˆ›å»ºæ•°æ®é›†å¡ç‰‡
        manager.create_dataset_card(
            dataset_name=dataset_name,
            description="åŒ»ç–—é—®ç­”æ•°æ®é›† - ç”±CAMEL CoTDataGeneratorç”Ÿæˆ",
            license="mit",
            language=["zh"],
            size_category="<1MB",
            version="0.1.0",
            tags=["camel", "medical", "question-answering", "chinese"],
            task_categories=["question-answering"],
            authors=[username]
        )
        
        # å°†æ•°æ®è½¬æ¢ä¸ºRecordå¯¹è±¡å¹¶æ·»åŠ åˆ°æ•°æ®é›†
        records = [Record(**item) for item in transformed_data]
        manager.add_records(dataset_name=dataset_name, records=records)
        
        return f"æ•°æ®é›†å·²æˆåŠŸä¸Šä¼ åˆ°HuggingFace: {dataset_url}"
    except Exception as e:
        return f"ä¸Šä¼ åˆ°HuggingFaceå¤±è´¥: {str(e)}"

def train_model(model_name, alpaca_file, epochs, learning_rate, output_dir="temp/trained_models"):
    """ä½¿ç”¨Unslothè®­ç»ƒæ¨¡å‹"""
    try:
        if not os.path.exists(alpaca_file):
            return "è®­ç»ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨"
        
        # å‡†å¤‡è®­ç»ƒæ•°æ®
        with open(alpaca_file, 'r', encoding='utf-8') as f:
            alpaca_data = json.load(f)
        
        # å°†æ•°æ®ä¿å­˜ä¸ºjsonlæ ¼å¼ç”¨äºåŠ è½½
        jsonl_file = alpaca_file.replace(".json", ".jsonl")
        with open(jsonl_file, 'w', encoding='utf-8') as f:
            for item in alpaca_data:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        max_seq_length = 2048
        dtype = None
        load_in_4bit = True
        
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_name,
            max_seq_length=max_seq_length,
            dtype=dtype,
            load_in_4bit=load_in_4bit,
        )
        
        # æ·»åŠ LoRAé€‚é…å™¨
        model = FastLanguageModel.get_peft_model(
            model,
            r=16,
            target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                           "gate_proj", "up_proj", "down_proj"],
            lora_alpha=16,
            lora_dropout=0,
            bias="none",
            use_gradient_checkpointing="unsloth",
            random_state=3407,
            use_rslora=False,
            loftq_config=None,
        )
        
        # å®šä¹‰æ ¼å¼åŒ–æç¤ºè¯å‡½æ•°
        alpaca_prompt = """ä¸‹é¢æ˜¯ä¸€ä¸ªæè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä»¥åŠæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚è¯·å†™ä¸€ä¸ªé€‚å½“åœ°å®Œæˆè¯·æ±‚çš„å›åº”ã€‚

### æŒ‡ä»¤:
{}

### è¾“å…¥:
{}

### å›åº”:
{}"""
        
        EOS_TOKEN = tokenizer.eos_token
        
        def formatting_prompts_func(examples):
            instructions = examples["instruction"]
            inputs = examples["input"]
            outputs = examples["output"]
            texts = []
            for instruction, input, output in zip(instructions, inputs, outputs):
                text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN
                texts.append(text)
            return {"text": texts}
        
        # åŠ è½½å¹¶å¤„ç†æ•°æ®é›†
        from datasets import load_dataset
        dataset = load_dataset("json", data_files=jsonl_file, split="train")
        dataset = dataset.map(formatting_prompts_func, batched=True)
        
        # è®¾ç½®è®­ç»ƒå‚æ•°
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_save_path = f"{output_dir}/{model_name.split('/')[-1]}_{timestamp}"
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = SFTTrainer(
            model=model,
            tokenizer=tokenizer,
            train_dataset=dataset,
            dataset_text_field="text",
            max_seq_length=max_seq_length,
            dataset_num_proc=2,
            packing=False,
            args=TrainingArguments(
                per_device_train_batch_size=2,
                gradient_accumulation_steps=4,
                warmup_steps=5,
                num_train_epochs=epochs,
                learning_rate=learning_rate,
                fp16=not is_bfloat16_supported(),
                bf16=is_bfloat16_supported(),
                logging_steps=1,
                optim="adamw_8bit",
                weight_decay=0.01,
                lr_scheduler_type="linear",
                seed=3407,
                output_dir=model_save_path,
                report_to="none",
            ),
        )
        
        # å¼€å§‹è®­ç»ƒ
        trainer_stats = trainer.train()
        
        # ä¿å­˜æ¨¡å‹
        model.save_pretrained(model_save_path)
        tokenizer.save_pretrained(model_save_path)
        
        training_time = trainer_stats.metrics['train_runtime']
        training_time_min = round(training_time / 60, 2)
        
        return f"æ¨¡å‹è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ {training_time_min} åˆ†é’Ÿï¼Œå·²ä¿å­˜åˆ° {model_save_path}"
    except Exception as e:
        return f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}"

def inference_with_model(model_path, question, input_text=""):
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œæ¨ç†"""
    try:
        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name=model_path,
            max_seq_length=2048,
            load_in_4bit=True,
        )
        
        # å¯ç”¨å¿«é€Ÿæ¨ç†
        FastLanguageModel.for_inference(model)
        
        # å‡†å¤‡æç¤ºè¯
        alpaca_prompt = """ä¸‹é¢æ˜¯ä¸€ä¸ªæè¿°ä»»åŠ¡çš„æŒ‡ä»¤ï¼Œä»¥åŠæä¾›è¿›ä¸€æ­¥ä¸Šä¸‹æ–‡çš„è¾“å…¥ã€‚è¯·å†™ä¸€ä¸ªé€‚å½“åœ°å®Œæˆè¯·æ±‚çš„å›åº”ã€‚

### æŒ‡ä»¤:
{}

### è¾“å…¥:
{}

### å›åº”:
"""
        
        # å‡†å¤‡è¾“å…¥
        prompt = alpaca_prompt.format(question, input_text)
        inputs = tokenizer([prompt], return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
        
        # ç”Ÿæˆå›ç­”
        outputs = model.generate(
            **inputs,
            max_new_tokens=1024,
            use_cache=True
        )
        
        # è§£ç è¾“å‡º
        decoded_output = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]
        
        # æå–å›ç­”éƒ¨åˆ†
        response = decoded_output.split("### å›åº”:")[1].strip()
        
        return response
    except Exception as e:
        return f"æ¨ç†å¤±è´¥: {str(e)}"

def save_qa_data_to_json(qa_data, file_path):
    """
    ä¿å­˜é—®ç­”æ•°æ®åˆ°JSONæ–‡ä»¶
    
    Args:
        qa_data (dict): é—®ç­”æ•°æ®å­—å…¸
        file_path (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
    """
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(os.path.dirname(file_path), exist_ok=True)
    
    # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼ŒåŠ è½½ç°æœ‰æ•°æ®å¹¶åˆå¹¶
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                existing_data = json.load(f)
            # åˆå¹¶æ–°æ—§æ•°æ®
            qa_data.update(existing_data)
        except json.JSONDecodeError:
            # å¦‚æœæ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œåˆ™è¦†ç›–
            pass
    
    # ä¿å­˜åˆå¹¶åçš„æ•°æ®
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(qa_data, f, ensure_ascii=False, indent=2)
    
    return f"æ•°æ®å·²ä¿å­˜åˆ° {file_path}"

def load_qa_data(file_path):
    """
    åŠ è½½QAæ•°æ®æ–‡ä»¶
    
    Args:
        file_path (str): æ–‡ä»¶è·¯å¾„
    
    Returns:
        str: æ ¼å¼åŒ–çš„QAæ•°æ®
    """
    try:
        if not os.path.exists(file_path):
            return "æ–‡ä»¶ä¸å­˜åœ¨"
        
        with open(file_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # æ ¼å¼åŒ–æ˜¾ç¤º
        formatted_data = json.dumps(data, ensure_ascii=False, indent=2)
        return formatted_data
    except Exception as e:
        return f"åŠ è½½å¤±è´¥: {str(e)}"

def export_qa_data(qa_source, output_path):
    """
    å¯¼å‡ºQAæ•°æ®åˆ°æŒ‡å®šè·¯å¾„
    
    Args:
        qa_source (str): æ•°æ®æ¥æºåç§° ("recommendation" æˆ– "test_report")
        output_path (str): ç›®æ ‡æ–‡ä»¶è·¯å¾„
    
    Returns:
        str: æ“ä½œç»“æœ
    """
    try:
        # æ ¹æ®æ•°æ®æ¥æºç¡®å®šæ–‡ä»¶è·¯å¾„
        if qa_source == "recommendation":
            file_path = "temp/recommendation_qa_data.json"
        elif qa_source == "test_report":
            file_path = "temp/test_report_qa_data.json"
        else:
            return "æ— æ•ˆçš„æ•°æ®æ¥æº"
        
        if not os.path.exists(file_path):
            return "æºæ–‡ä»¶ä¸å­˜åœ¨"
        
        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # å¤åˆ¶æ–‡ä»¶
        with open(file_path, "r", encoding="utf-8") as src:
            data = json.load(src)
        
        with open(output_path, "w", encoding="utf-8") as dst:
            json.dump(data, dst, ensure_ascii=False, indent=2)
        
        return f"æ•°æ®å·²æˆåŠŸå¯¼å‡ºåˆ° {output_path}"
    except Exception as e:
        return f"å¯¼å‡ºå¤±è´¥: {str(e)}"

def convert_qa_to_training_format(file_path, output_path=None):
    """
    å°†QAæ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ‰€éœ€çš„æ ¼å¼
    
    Args:
        file_path (str): æºæ–‡ä»¶è·¯å¾„
        output_path (str, optional): è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
    
    Returns:
        str: æ“ä½œç»“æœå’Œè¾“å‡ºè·¯å¾„
    """
    try:
        if not os.path.exists(file_path):
            return "æºæ–‡ä»¶ä¸å­˜åœ¨"
        
        # è¯»å–QAæ•°æ®
        with open(file_path, "r", encoding="utf-8") as f:
            qa_data = json.load(f)
        
        # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
        training_data = []
        for question, answer in qa_data.items():
            training_data.append({
                "instruction": question,
                "input": "",
                "output": answer
            })
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        if output_path is None:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"export/training_data_{timestamp}.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ä¿å­˜ä¸ºè®­ç»ƒæ ¼å¼
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(training_data, f, ensure_ascii=False, indent=2)
        
        return f"æ•°æ®å·²è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼å¹¶ä¿å­˜åˆ° {output_path}", output_path
    except Exception as e:
        return f"è½¬æ¢å¤±è´¥: {str(e)}", None

def merge_qa_datasets(file_paths, output_path=None):
    """
    åˆå¹¶å¤šä¸ªQAæ•°æ®é›†
    
    Args:
        file_paths (list): æºæ–‡ä»¶è·¯å¾„åˆ—è¡¨
        output_path (str, optional): è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ
    
    Returns:
        str: æ“ä½œç»“æœ
    """
    try:
        if not file_paths:
            return "æœªæä¾›æ–‡ä»¶åˆ—è¡¨"
        
        # è¯»å–å¹¶åˆå¹¶æ‰€æœ‰æ•°æ®
        merged_data = {}
        for file_path in file_paths:
            if os.path.exists(file_path):
                with open(file_path, "r", encoding="utf-8") as f:
                    data = json.load(f)
                merged_data.update(data)
        
        if not merged_data:
            return "æ²¡æœ‰æœ‰æ•ˆæ•°æ®å¯åˆå¹¶"
        
        # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶è·¯å¾„
        if not output_path:
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            output_path = f"export/merged_qa_data_{timestamp}.json"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        
        # ä¿å­˜åˆå¹¶çš„æ•°æ®
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        return f"å·²æˆåŠŸåˆå¹¶ {len(merged_data)} æ¡QAæ•°æ®å¹¶ä¿å­˜åˆ° {output_path}"
    except Exception as e:
        return f"åˆå¹¶å¤±è´¥: {str(e)}"

# Add a completely new QA Data Management tab function

def create_qa_data_management_tab():
    """
    åˆ›å»ºQAæ•°æ®ç®¡ç†æ ‡ç­¾é¡µ
    """
    with gr.Tab("ğŸ“Š åŒ»ç–—æ•°æ®ç®¡ç†"):
        gr.Markdown("### æ£€éªŒæ•°æ®çŸ¥è¯†åº“ç®¡ç†")
        gr.Markdown('<div class="medical-alert">æœ¬æ¨¡å—ç”¨äºç®¡ç†ç³»ç»Ÿäº§ç”Ÿçš„åŒ»ç–—é—®ç­”æ•°æ®ï¼Œå¯å¯¼å‡ºç”¨äºè®­ç»ƒå’Œæ”¹è¿›AIæ¨¡å‹</div>')
        
        with gr.Tabs():
            with gr.TabItem("æ£€éªŒé¡¹ç›®æ¨èæ•°æ®"):
                with gr.Row():
                    with gr.Column():
                        with gr.Box(elem_classes="block"):
                            gr.Markdown("#### æ•°æ®æ“ä½œ")
                            inspection_qa_view_btn = gr.Button("æŸ¥çœ‹æ£€éªŒæ¨èæ•°æ®", variant="primary", elem_classes="primary-btn")
                            inspection_qa_export_path = gr.Textbox(
                                label="å¯¼å‡ºè·¯å¾„", 
                                value="export/inspection_qa_data.json",
                                placeholder="è¾“å…¥å¯¼å‡ºæ–‡ä»¶è·¯å¾„"
                            )
                            with gr.Row():
                                inspection_qa_export_btn = gr.Button("å¯¼å‡ºæ•°æ®", variant="secondary")
                                inspection_qa_convert_btn = gr.Button("è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼", variant="secondary")
                    
                    with gr.Column():
                        inspection_qa_data = gr.Textbox(label="æ•°æ®å†…å®¹", lines=20)
                        inspection_qa_status = gr.Textbox(label="æ“ä½œçŠ¶æ€", lines=1)
            
            with gr.TabItem("æ£€éªŒæŠ¥å‘Šæ•°æ®"):
                with gr.Row():
                    with gr.Column():
                        with gr.Box(elem_classes="block"):
                            gr.Markdown("#### æ•°æ®æ“ä½œ")
                            report_qa_view_btn = gr.Button("æŸ¥çœ‹æ£€éªŒæŠ¥å‘Šæ•°æ®", variant="primary", elem_classes="primary-btn")
                            report_qa_export_path = gr.Textbox(
                                label="å¯¼å‡ºè·¯å¾„", 
                                value="export/report_qa_data.json",
                                placeholder="è¾“å…¥å¯¼å‡ºæ–‡ä»¶è·¯å¾„"
                            )
                            with gr.Row():
                                report_qa_export_btn = gr.Button("å¯¼å‡ºæ•°æ®", variant="secondary")
                                report_qa_convert_btn = gr.Button("è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼", variant="secondary")
                    
                    with gr.Column():
                        report_qa_data = gr.Textbox(label="æ•°æ®å†…å®¹", lines=20)
                        report_qa_status = gr.Textbox(label="æ“ä½œçŠ¶æ€", lines=1)
            
            with gr.TabItem("æ•°æ®é›†åˆå¹¶"):
                with gr.Row():
                    with gr.Column():
                        with gr.Box(elem_classes="block"):
                            gr.Markdown("#### æ•°æ®é›†åˆå¹¶é…ç½®")
                            merge_files_checkbox = gr.CheckboxGroup(
                                choices=[
                                    "temp/recommendation_qa_data.json", 
                                    "temp/test_report_qa_data.json"
                                ],
                                label="é€‰æ‹©è¦åˆå¹¶çš„æ•°æ®é›†",
                                value=["temp/recommendation_qa_data.json", "temp/test_report_qa_data.json"]
                            )
                            merge_output_path = gr.Textbox(
                                label="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰",
                                placeholder="ç•™ç©ºå°†è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å",
                                value=""
                            )
                            with gr.Row():
                                merge_datasets_btn = gr.Button("åˆå¹¶æ•°æ®é›†", variant="primary", elem_classes="primary-btn")
                                merge_convert_btn = gr.Button("åˆå¹¶å¹¶è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼", variant="secondary")
                    
                    with gr.Column():
                        with gr.Box(elem_classes="block"):
                            gr.Markdown("#### åˆå¹¶ç»“æœ")
                            merge_status = gr.Textbox(label="çŠ¶æ€ä¿¡æ¯", lines=3)
                            merged_data_view = gr.Textbox(label="æ•°æ®é¢„è§ˆ", lines=15)
        
        # è®¾ç½®äº‹ä»¶
        inspection_qa_view_btn.click(
            fn=lambda: load_qa_data("temp/recommendation_qa_data.json"),
            inputs=None,
            outputs=inspection_qa_data
        )
        
        def export_recommendation_data(output_path):
            """å¯¼å‡ºæ£€éªŒæ¨èQAæ•°æ®"""
            return export_qa_data("recommendation", output_path)
        
        inspection_qa_export_btn.click(
            fn=export_recommendation_data,
            inputs=inspection_qa_export_path,
            outputs=inspection_qa_status
        )
        
        def convert_recommendation_data():
            """è½¬æ¢æ£€éªŒæ¨èQAæ•°æ®ä¸ºè®­ç»ƒæ ¼å¼"""
            result, _ = convert_qa_to_training_format("temp/recommendation_qa_data.json")
            return result
        
        inspection_qa_convert_btn.click(
            fn=convert_recommendation_data,
            inputs=None,
            outputs=inspection_qa_status
        )
        
        report_qa_view_btn.click(
            fn=lambda: load_qa_data("temp/test_report_qa_data.json"),
            inputs=None,
            outputs=report_qa_data
        )
        
        def export_report_data(output_path):
            """å¯¼å‡ºæ£€éªŒæŠ¥å‘ŠQAæ•°æ®"""
            return export_qa_data("test_report", output_path)
        
        report_qa_export_btn.click(
            fn=export_report_data,
            inputs=report_qa_export_path,
            outputs=report_qa_status
        )
        
        def convert_report_data():
            """è½¬æ¢æ£€éªŒæŠ¥å‘ŠQAæ•°æ®ä¸ºè®­ç»ƒæ ¼å¼"""
            result, _ = convert_qa_to_training_format("temp/test_report_qa_data.json")
            return result
        
        report_qa_convert_btn.click(
            fn=convert_report_data,
            inputs=None,
            outputs=report_qa_status
        )
        
        merge_datasets_btn.click(
            fn=merge_qa_datasets,
            inputs=[merge_files_checkbox, merge_output_path],
            outputs=merge_status
        )
        
        def run_merge_and_convert(file_paths, output_path):
            """è¿è¡Œåˆå¹¶å’Œè½¬æ¢æ“ä½œ"""
            if not file_paths:
                return "æœªæä¾›æ–‡ä»¶åˆ—è¡¨", ""
            
            # å¤„ç†è¾“å‡ºè·¯å¾„
            if not output_path:
                timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                merged_path = f"export/merged_qa_data_{timestamp}.json"
            else:
                merged_path = output_path
            
            # åˆå¹¶æ•°æ®
            merge_result = merge_qa_datasets(file_paths, merged_path)
            
            # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
            training_result, _ = convert_qa_to_training_format(merged_path)
            
            # åŠ è½½åˆå¹¶åçš„æ•°æ®
            preview = load_qa_data(merged_path)
            
            return f"{merge_result}\n{training_result}", preview
        
        merge_convert_btn.click(
            fn=run_merge_and_convert,
            inputs=[merge_files_checkbox, merge_output_path],
            outputs=[merge_status, merged_data_view]
        )
        
        return {
            "inspection_qa_data": inspection_qa_data,
            "inspection_qa_status": inspection_qa_status,
            "report_qa_data": report_qa_data,
            "report_qa_status": report_qa_status,
            "merge_status": merge_status,
            "merged_data_view": merged_data_view
        }

# Replace the existing "10. QAæ•°æ®ç®¡ç†" tab with a call to the function
# Find this section in the Gradio interface definition and replace it with:
    qa_data_components = create_qa_data_management_tab()

# åˆ›å»ºå…¨å±€å…±äº«çŠ¶æ€
shared_text_state = gr.State("")

# åˆ›å»ºGradioç•Œé¢
with gr.Blocks(title="åŒ»æ£€AIå¹³å°", theme=MEDICAL_THEME, css=CUSTOM_CSS) as app:
    gr.Markdown("# åŒ»é™¢AIagentæ™ºèƒ½æ£€éªŒå¹³å°ï¼šåŸºäºcamelå¤šæ™ºèƒ½ä½“æ¡†æ¶å’ŒåŒ»æ£€æ¨ç†å°æ¨¡å‹")
    gr.Markdown('<div class="medical-success">æ¬¢è¿ä½¿ç”¨åŒ»é™¢æ™ºèƒ½æ£€éªŒå¹³å° - ä¸ºä¸´åºŠåŒ»ç”Ÿå’ŒåŒ»æŠ€äººå‘˜æä¾›AIè¾…åŠ©è¯Šæ–­å’Œæ£€éªŒåˆ†æå·¥å…·</div>')
    
    # åˆ›å»ºå…±äº«çŠ¶æ€
    conversation_text = gr.State("")
    
    with gr.Tab("ğŸ“ è¯Šå®¤å¯¹è¯è®°å½•"):
        gr.Markdown('<div class="medical-success">å½•åˆ¶æ‚£è€…å¯¹è¯æˆ–ä¸Šä¼ å½•éŸ³ï¼Œç³»ç»Ÿå°†è‡ªåŠ¨è½¬æ¢ä¸ºæ–‡æœ¬ï¼Œæ–¹ä¾¿è¯Šæ–­è®°å½•</div>')
        
        with gr.Row():
            with gr.Column():
                with gr.Box(elem_classes="block"):
                    gr.Markdown("#### éŸ³é¢‘è·å–æ–¹å¼")
                    with gr.Tabs():
                        with gr.TabItem("å®æ—¶å½•éŸ³"):
                            audio_recorder = gr.Audio(label="ç‚¹å‡»éº¦å…‹é£å›¾æ ‡å¼€å§‹å½•éŸ³", source="microphone", type="filepath")
                            start_recording_btn = gr.Button("å¼€å§‹å½•éŸ³", variant="primary", elem_classes="primary-btn")
                            gr.Markdown('<div class="medical-alert">ğŸ’¡ æç¤ºï¼šå½•éŸ³å®Œæˆåè‡ªåŠ¨è½¬æ¢ï¼Œæˆ–ç‚¹å‡»å¤„ç†éŸ³é¢‘æŒ‰é’®æ‰‹åŠ¨è½¬æ¢</div>')
                            
                        with gr.TabItem("ä¸Šä¼ éŸ³é¢‘"):
                            audio_input = gr.Audio(label="ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶", type="filepath")
                    
                    with gr.Row():
                        audio_process_btn = gr.Button("å¤„ç†éŸ³é¢‘", variant="primary", elem_classes="primary-btn")
                        clear_audio_btn = gr.Button("æ¸…é™¤", variant="secondary")
            
            with gr.Column():
                with gr.Box(elem_classes="block"):
                    gr.Markdown("#### è½¬æ¢ç»“æœ")
                    text_output = gr.Textbox(label="è½¬æ¢ç»“æœ", lines=15)
                    with gr.Row():
                        save_text_btn = gr.Button("ä¿å­˜è½¬æ¢ç»“æœ", variant="secondary")
                        copy_to_recommend_btn = gr.Button("å¤åˆ¶åˆ°æ£€éªŒæ¨è", variant="secondary")
        
        gr.Markdown("---")
        gr.Markdown("### æ–‡æœ¬è½¬è¯­éŸ³")
        
        with gr.Row():
            with gr.Column():
                with gr.Box(elem_classes="block"):
                    gr.Markdown("#### è¾“å…¥å†…å®¹")
                    text_input = gr.Textbox(label="è¾“å…¥æ–‡æœ¬ç”Ÿæˆè¯­éŸ³", lines=5, placeholder="è¾“å…¥éœ€è¦è½¬æ¢ä¸ºè¯­éŸ³çš„åŒ»å˜±æˆ–å»ºè®®...")
                    text_to_speech_btn = gr.Button("ç”Ÿæˆè¯­éŸ³", variant="primary", elem_classes="primary-btn")
            
            with gr.Column():
                with gr.Box(elem_classes="block"):
                    gr.Markdown("#### åˆæˆè¯­éŸ³")
                    audio_output = gr.Audio(label="ç”Ÿæˆçš„è¯­éŸ³")
                    audio_status = gr.Textbox(label="çŠ¶æ€")
        
        # è®¾ç½®äº‹ä»¶
        def process_recorded_or_uploaded_audio(audio_file_path):
            if audio_file_path:
                return process_audio(audio_file_path)
            else:
                return "è¯·å…ˆå½•åˆ¶æˆ–ä¸Šä¼ éŸ³é¢‘æ–‡ä»¶"
        
        def save_text_to_file(text):
            if not text:
                return "æ²¡æœ‰å¯ä¿å­˜çš„æ–‡æœ¬å†…å®¹"
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = f"temp/conversation_{timestamp}.txt"
            try:
                with open(save_path, "w", encoding="utf-8") as f:
                    f.write(text)
                return f"æ–‡æœ¬å·²ä¿å­˜åˆ° {save_path}"
            except Exception as e:
                return f"ä¿å­˜å¤±è´¥: {str(e)}"
        
        def clear_audio():
            return None, None, ""
                
        # ç»‘å®šå½•éŸ³å’Œä¸Šä¼ çš„å¤„ç†å‡½æ•°
        audio_input.change(
            fn=process_recorded_or_uploaded_audio,
            inputs=audio_input,
            outputs=text_output
        )
        
        audio_recorder.change(
            fn=process_recorded_or_uploaded_audio,
            inputs=audio_recorder,
            outputs=text_output
        )
        
        # æ·»åŠ å½•éŸ³æŒ‰é’®åŠŸèƒ½
        start_recording_btn.click(
            fn=lambda: None,  # ä¸åšä»»ä½•å¤„ç†ï¼Œåªæ˜¯æç¤ºç”¨æˆ·ç‚¹å‡»éº¦å…‹é£å›¾æ ‡
            inputs=[],
            outputs=[]
        )
        
        audio_process_btn.click(
            fn=lambda x, y: process_recorded_or_uploaded_audio(x or y),
            inputs=[audio_input, audio_recorder],
            outputs=text_output
        )
        
        clear_audio_btn.click(
            fn=clear_audio,
            inputs=[],
            outputs=[audio_input, audio_recorder, text_output]
        )
        
        save_text_btn.click(
            fn=save_text_to_file,
            inputs=text_output,
            outputs=audio_status
        )
        
        # æ·»åŠ å¤åˆ¶åˆ°æ£€éªŒæ¨èåŠŸèƒ½
        shared_text_state = gr.State("")
        
        def copy_to_recommendation(text, state):
            """å°†æ–‡æœ¬å¤åˆ¶åˆ°å…±äº«çŠ¶æ€ï¼Œå¹¶é€šçŸ¥ç”¨æˆ·åˆ‡æ¢é€‰é¡¹å¡"""
            if not text:
                return "æ²¡æœ‰å¯å¤åˆ¶çš„æ–‡æœ¬", state
            return "æ–‡æœ¬å·²å‡†å¤‡å¥½ï¼Œè¯·åˆ‡æ¢åˆ°æ£€éªŒé¡¹ç›®æ¨èé€‰é¡¹å¡", text
        
        copy_to_recommend_btn.click(
            fn=copy_to_recommendation,
            inputs=[text_output, conversation_text],
            outputs=[audio_status, conversation_text]
        )
        
        text_to_speech_btn.click(fn=text_to_audio, inputs=text_input, outputs=[audio_output, audio_status])
    
    with gr.Tab("ğŸ”¬ æ£€éªŒé¡¹ç›®æ¨è"):
        with gr.Row():
            with gr.Column():
                with gr.Box(elem_classes="block"):
                    gr.Markdown("#### æ‚£è€…å¯¹è¯å†…å®¹")
                    load_text_btn = gr.Button("åŠ è½½è½¬æ¢çš„æ–‡æœ¬", variant="secondary")
                    conversation_input = gr.Textbox(
                        label="åŒ»æ‚£å¯¹è¯å†…å®¹", 
                        lines=15,
                        elem_id="conversation_input"
                    )
                    recommend_btn = gr.Button("æ¨èæ£€éªŒé¡¹ç›®", variant="primary", elem_classes="primary-btn")
            with gr.Column():
                with gr.Box(elem_classes="block"):
                    gr.Markdown("#### æ¨èæ£€éªŒé¡¹ç›®")
                    recommendation_output = gr.Textbox(label="æ¨èç»“æœ", lines=20)
                    save_recommendation_btn = gr.Button("ä¿å­˜æ¨èç»“æœ", variant="secondary")
        
        # è®¾ç½®äº‹ä»¶
        load_text_btn.click(
            fn=lambda state: state if state else load_converted_text(),
            inputs=[conversation_text],
            outputs=conversation_input
        )
        
        # ä»å¯¹è¯é¡µé¢è·å–æ•°æ®
        def load_shared_text(state):
            if state:
                return state
            return gr.update()
        
        # å®šä¹‰é€‰é¡¹å¡åˆ‡æ¢æ—¶çš„å¤„ç†å‡½æ•°
        def on_tab_change(tab_index):
            # å½“åˆ‡æ¢åˆ°æ£€éªŒé¡¹ç›®æ¨èé¡µé¢(ç´¢å¼•ä¸º1)æ—¶ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®éœ€è¦åŠ è½½
            if tab_index == 1:  # æ£€éªŒé¡¹ç›®æ¨èé¡µé¢çš„ç´¢å¼•
                return load_shared_text(conversation_text.value)
            return gr.update()
        
        # æ³¨å†Œé€‰é¡¹å¡å˜æ›´äº‹ä»¶
        app.load(
            fn=load_shared_text,
            inputs=[conversation_text],
            outputs=conversation_input
        )
        
        recommend_btn.click(fn=recommend_inspection_items, inputs=conversation_input, outputs=recommendation_output)
        
        # ä¿å­˜æ¨èç»“æœ
        def save_recommendation(text):
            if not text:
                return "æ²¡æœ‰å¯ä¿å­˜çš„æ¨èç»“æœ"
            timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
            save_path = f"temp/recommendation_{timestamp}.txt"
            try:
                with open(save_path, "w", encoding="utf-8") as f:
                    f.write(text)
                return f"æ¨èç»“æœå·²ä¿å­˜åˆ° {save_path}"
            except Exception as e:
                return f"ä¿å­˜å¤±è´¥: {str(e)}"
        
        save_recommendation_btn.click(
            fn=save_recommendation,
            inputs=recommendation_output,
            outputs=gr.Textbox(elem_id="recommendation_status", visible=False) # éšè—çŠ¶æ€æ¡†
        )
    
    with gr.Tab("ğŸ” æ£€éªŒç»“æœåˆ†æ"):
        with gr.Row():
            with gr.Column():
                image_inputs = gr.File(label="ä¸Šä¼ æ£€éªŒç»“æœå›¾ç‰‡", file_count="multiple")
                other_info = gr.Textbox(label="å…¶ä»–æ‚£è€…ä¿¡æ¯", lines=10)
                analyze_images_btn = gr.Button("åˆ†æå›¾åƒ", variant="primary", elem_classes="primary-btn")
            with gr.Column():
                image_analysis_output = gr.Textbox(label="åˆ†æçŠ¶æ€", lines=5)
        
        # è®¾ç½®äº‹ä»¶
        analyze_images_btn.click(fn=analyze_test_images, inputs=[image_inputs, other_info], outputs=image_analysis_output)
    
    with gr.Tab("ğŸ“‹ æ£€éªŒæŠ¥å‘Šç”Ÿæˆ"):
        with gr.Row():
            with gr.Column():
                generate_report_btn = gr.Button("ç”Ÿæˆæ£€éªŒæŠ¥å‘Š", variant="primary", elem_classes="primary-btn")
            with gr.Column():
                report_output = gr.Textbox(label="ç”Ÿæˆçš„æŠ¥å‘Š", lines=25)
        
        # è®¾ç½®äº‹ä»¶
        generate_report_btn.click(fn=generate_test_report, inputs=None, outputs=report_output)
    
    with gr.Tab("ğŸ“Š æ•°æ®åˆ†æä¸ç ”ç©¶"):
        with gr.Row():
            with gr.Column():
                csv_input = gr.File(label="ä¸Šä¼ CSVæ•°æ®æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰", file_types=[".csv"])
                analysis_requirements = gr.Textbox(
                    label="åˆ†æéœ€æ±‚", 
                    lines=10,
                    value=data_analysis.example_user_info
                )
                analyze_data_btn = gr.Button("åˆ†ææ•°æ®", variant="primary", elem_classes="primary-btn")
            with gr.Column():
                data_analysis_output = gr.Textbox(label="åˆ†æç»“æœ", lines=25)
        
        # è®¾ç½®äº‹ä»¶
        analyze_data_btn.click(fn=analyze_csv_data, inputs=[csv_input, analysis_requirements], outputs=data_analysis_output)
    
    with gr.Tab("ğŸ§ª ä¸´åºŠæ•°æ®åˆæˆ"):
        with gr.Row():
            with gr.Column():
                main_topic = gr.Textbox(label="ä¸»é¢˜", value="è‚åŠŸèƒ½æ£€éªŒæ•°æ®")
                with gr.Row():
                    total_examples = gr.Slider(label="ç”Ÿæˆç¤ºä¾‹æ•°é‡", minimum=5, maximum=50, value=10, step=5)
                    num_subtopics = gr.Slider(label="å­ä¸»é¢˜æ•°é‡", minimum=1, maximum=10, value=5, step=1)
                temperature = gr.Slider(label="æ¨¡å‹æ¸©åº¦", minimum=0.1, maximum=1.0, value=0.2, step=0.1)
                generate_data_btn = gr.Button("ç”Ÿæˆæ•°æ®", variant="primary", elem_classes="primary-btn")
            with gr.Column():
                synthetic_data_output = gr.Textbox(label="ç”Ÿæˆçš„æ•°æ®", lines=25)
        
        # è®¾ç½®äº‹ä»¶
        generate_data_btn.click(
            fn=generate_synthetic_data, 
            inputs=[main_topic, total_examples, num_subtopics, temperature], 
            outputs=synthetic_data_output
        )
    
    with gr.Tab("ğŸ’¬ ç—…ä¾‹å¯¹è¯ç”Ÿæˆ"):
        with gr.Row():
            with gr.Column():
                patient_history_input = gr.Textbox(
                    label="è¾“å…¥æ‚£è€…ç—…å†", 
                    lines=15,
                    placeholder="è¯·åœ¨æ­¤è¾“å…¥æ‚£è€…ç—…å†ä¿¡æ¯ï¼Œç³»ç»Ÿå°†æ ¹æ®ç—…å†ç”ŸæˆåŒ»æ‚£å¯¹è¯..."
                )
                generate_dialogue_btn = gr.Button("ç”ŸæˆåŒ»æ‚£å¯¹è¯", variant="primary", elem_classes="primary-btn")
            with gr.Column():
                dialogue_output = gr.Textbox(label="ç”Ÿæˆçš„åŒ»æ‚£å¯¹è¯", lines=25)
        
        # è®¾ç½®äº‹ä»¶
        generate_dialogue_btn.click(
            fn=generate_doctor_patient_dialogue, 
            inputs=patient_history_input, 
            outputs=dialogue_output
        )
    
    with gr.Tab("âš™ï¸ ç³»ç»Ÿæ€§èƒ½è¯„æµ‹"):
        gr.Markdown("### ç³»ç»Ÿæ€§èƒ½è¯„æµ‹")
        gr.Markdown('<div class="medical-alert">æœ¬æ¨¡å—ç”¨äºè¯„ä¼°AIç³»ç»Ÿå„ç»„ä»¶æ€§èƒ½ï¼Œç¡®ä¿æ»¡è¶³åŒ»ç–—åº”ç”¨è´¨é‡æ ‡å‡†</div>')
        
        with gr.Row():
            with gr.Column():
                with gr.Box(elem_classes="block"):
                    gr.Markdown("#### è¯„æµ‹é…ç½®")
                    benchmark_components = gr.CheckboxGroup(
                        choices=["è¯­éŸ³è½¬æ–‡æœ¬", "æ£€éªŒé¡¹ç›®æ¨è", "å›¾åƒåˆ†æ", "æ£€éªŒæŠ¥å‘Šç”Ÿæˆ", "æ•°æ®åˆ†æ", "åˆæˆæ•°æ®ç”Ÿæˆ", "åŒ»æ‚£å¯¹è¯ç”Ÿæˆ"],
                        label="é€‰æ‹©è¦æµ‹è¯•çš„ç»„ä»¶",
                        value=["è¯­éŸ³è½¬æ–‡æœ¬", "æ£€éªŒé¡¹ç›®æ¨è", "å›¾åƒåˆ†æ", "åŒ»æ‚£å¯¹è¯ç”Ÿæˆ"]
                    )
                    num_test_samples = gr.Slider(
                        minimum=1, 
                        maximum=5, 
                        value=3, 
                        step=1,
                        label="æµ‹è¯•æ ·æœ¬æ•°é‡"
                    )
                    with gr.Row():
                        benchmark_btn = gr.Button("è¿è¡Œæ€§èƒ½è¯„æµ‹", variant="primary", elem_classes="primary-btn")
                        download_report_btn = gr.Button("ä¸‹è½½å®Œæ•´HTMLæŠ¥å‘Š", variant="secondary")
            
            with gr.Column():
                with gr.Tabs():
                    with gr.TabItem("è¡¨æ ¼ç»“æœ"):
                        benchmark_table = gr.Dataframe(
                            headers=["æ¨¡å—", "å¤„ç†æ—¶é—´(ç§’)", "å†…å­˜ä½¿ç”¨(MB)", "æˆåŠŸç‡(%)", "è´¨é‡è¯„åˆ†"],
                            label="æ€§èƒ½è¯„æµ‹ç»“æœ"
                        )
                    with gr.TabItem("å›¾è¡¨ç»“æœ"):
                        benchmark_plot = gr.Image(label="æ€§èƒ½è¯„æµ‹å›¾è¡¨")
                
                with gr.Box(elem_classes="block"):
                    gr.Markdown("#### è¯„æµ‹è¯´æ˜")
                    benchmark_notes = gr.Markdown("""
                    **æ€§èƒ½æŒ‡æ ‡è§£è¯»:**
                    
                    - **å¤„ç†æ—¶é—´**: æ¯ä¸ªæ¨¡å—å¤„ç†å•ä¸ªæ ·æœ¬çš„å¹³å‡æ—¶é—´ï¼ˆç§’ï¼‰
                    - **å†…å­˜ä½¿ç”¨**: ä¼°è®¡çš„å†…å­˜æ¶ˆè€—ï¼ˆMBï¼‰
                    - **æˆåŠŸç‡**: æˆåŠŸå¤„ç†æ ·æœ¬çš„ç™¾åˆ†æ¯”
                    - **è´¨é‡è¯„åˆ†**: è¾“å‡ºç»“æœçš„è´¨é‡è¯„åˆ†ï¼ˆ0-100ï¼‰
                    
                    æ€§èƒ½è¯„æµ‹æœ‰åŠ©äºè¯†åˆ«ç³»ç»Ÿç“¶é¢ˆï¼Œä¼˜åŒ–ä¸´åºŠåº”ç”¨æ•ˆç‡ã€‚å®Œæ•´è¯„æµ‹æŠ¥å‘ŠåŒ…å«è¯¦ç»†åˆ†æå’Œæ”¹è¿›å»ºè®®ã€‚
                    """)
                
                benchmark_report_path = gr.Textbox(
                    label="HTMLæŠ¥å‘Šè·¯å¾„", 
                    value="./temp/benchmark_results/benchmark_report.html",
                    visible=False
                )
        
        # è®¾ç½®äº‹ä»¶
        benchmark_btn.click(
            fn=run_benchmark_from_gradio, 
            inputs=[benchmark_components, num_test_samples], 
            outputs=[benchmark_table, benchmark_plot]
        )
        
        def open_html_report(report_path):
            import webbrowser
            try:
                webbrowser.open(report_path)
                return "å·²åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€æŠ¥å‘Š"
            except Exception as e:
                return f"æ‰“å¼€æŠ¥å‘Šå¤±è´¥: {str(e)}"
        
        download_report_btn.click(
            fn=open_html_report,
            inputs=benchmark_report_path,
            outputs=benchmark_notes
        )
    
    with gr.Tab("ğŸ¤– æ¨¡å‹å¾®è°ƒè®­ç»ƒ"):
        gr.Markdown("### åŒ»ç–—AIæ¨¡å‹è®­ç»ƒä¸­å¿ƒ")
        gr.Markdown('<div class="medical-alert">æœ¬æ¨¡å—ç”¨äºè®­ç»ƒå’Œå¾®è°ƒåŒ»ç–—ä¸“ç”¨AIæ¨¡å‹ï¼Œä¸ºä¸´åºŠæ£€éªŒå’Œè¯Šæ–­æä¾›æ›´ç²¾ç¡®çš„æ”¯æŒ</div>')
        
        with gr.Accordion("åŒ»ç–—æ€ç»´é“¾æ•°æ®ç”Ÿæˆ", open=True):
            with gr.Row():
                with gr.Column():
                    with gr.Box(elem_classes="block"):
                        gr.Markdown("#### APIé…ç½®")
                        openai_api_key = gr.Textbox(label="APIå¯†é’¥", type="password")
                        model_choice = gr.Dropdown(
                            choices=["gpt-4.1", "gpt-4o-mini", "gpt-4o"], 
                            label="é€‰æ‹©åŸºç¡€æ¨¡å‹", 
                            value="gpt-3.5-turbo"
                        )
                        system_message = gr.Textbox(
                            label="ä¸“ä¸šæç¤ºè¯", 
                            value="ä½ æ˜¯ä¸€ä½åŒ»å­¦ä¸“å®¶ï¼Œæ“…é•¿è¿›è¡Œæ·±å…¥æ€è€ƒå¹¶ç»™å‡ºè¯¦ç»†çš„åŒ»å­¦åˆ†æ", 
                            lines=2
                        )
                        create_agent_btn = gr.Button("åˆ›å»ºæ™ºèƒ½ä½“", variant="primary", elem_classes="primary-btn")
                        agent_status = gr.Textbox(label="çŠ¶æ€", lines=1)
            
            gr.Markdown("---")
            gr.Markdown("### åŒ»ç–—é—®ç­”æ•°æ®")
            
            with gr.Row():
                with gr.Column():
                    with gr.Box(elem_classes="block"):
                        qa_data = gr.Textbox(
                            label="ä¸´åºŠé—®ç­”æ•°æ®ï¼ˆJSONæ ¼å¼ï¼‰", 
                            lines=10,
                            placeholder="""{"é«˜è¡€å‹æœ‰å“ªäº›æ²»ç–—æ–¹æ³•ï¼Ÿ": "é«˜è¡€å‹æ˜¯ä¸€ç§å¸¸è§æ…¢æ€§ç–¾ç—…...", "ç³–å°¿ç—…çš„æ—©æœŸç—‡çŠ¶æœ‰å“ªäº›ï¼Ÿ": "ç³–å°¿ç—…çš„æ—©æœŸç—‡çŠ¶åŒ…æ‹¬..."}"""
                        )
                        qa_data_file = gr.File(label="æˆ–ä¸Šä¼ JSONæ–‡ä»¶", file_types=[".json"])
                        generate_cot_btn = gr.Button("ç”Ÿæˆæ€ç»´é“¾æ•°æ®", variant="primary", elem_classes="primary-btn")
                
                with gr.Column():
                    cot_output = gr.Textbox(label="ç”Ÿæˆç»“æœ", lines=15)
                    alpaca_file_path = gr.Textbox(label="è®­ç»ƒæ ¼å¼æ–‡ä»¶è·¯å¾„", visible=False)
            
            gr.Markdown("---")
            gr.Markdown("### ä¸Šä¼ åˆ°æ¨¡å‹åº“")
            
            with gr.Row():
                with gr.Column():
                    with gr.Box(elem_classes="block"):
                        hf_token = gr.Textbox(label="è®¿é—®ä»¤ç‰Œ", type="password")
                        hf_username = gr.Textbox(label="ç”¨æˆ·å")
                        hf_dataset_name = gr.Textbox(label="æ•°æ®é›†åç§°ï¼ˆå¯é€‰ï¼‰")
                        upload_hf_btn = gr.Button("ä¸Šä¼ åˆ°æ•°æ®åº“", variant="primary", elem_classes="primary-btn")
                
                with gr.Column():
                    upload_status = gr.Textbox(label="ä¸Šä¼ çŠ¶æ€", lines=3)
        
        with gr.Accordion("ä¸“ä¸šæ¨¡å‹è®­ç»ƒ", open=True):
            with gr.Row():
                with gr.Column():
                    with gr.Box(elem_classes="block"):
                        gr.Markdown("#### è®­ç»ƒé…ç½®")
                        sft_model_name = gr.Dropdown(
                            choices=[
                                "unsloth/Qwen2.5-0.5B", 
                                "unsloth/Qwen2.5-1.5B", 
                                "unsloth/Qwen2.5-3B",
                                "unsloth/Qwen2.5-7B",
                                "unsloth/Qwen2.5-14B"
                            ],
                            label="é€‰æ‹©åŸºç¡€æ¨¡å‹",
                            value="unsloth/Qwen2.5-1.5B"
                        )
                        with gr.Row():
                            training_epochs = gr.Slider(label="è®­ç»ƒè½®æ¬¡", minimum=1, maximum=10, value=3, step=1)
                            learning_rate = gr.Slider(label="å­¦ä¹ ç‡", minimum=1e-5, maximum=5e-4, value=2e-4, step=1e-5)
                        train_btn = gr.Button("å¼€å§‹è®­ç»ƒ", variant="primary", elem_classes="primary-btn")
                
                with gr.Column():
                    with gr.Box(elem_classes="block"):
                        gr.Markdown("#### è®­ç»ƒè¿›åº¦")
                        training_status = gr.Textbox(label="è®­ç»ƒçŠ¶æ€", lines=5)
        
        with gr.Accordion("ä¸´åºŠé—®ç­”æµ‹è¯•", open=True):
            with gr.Row():
                with gr.Column():
                    with gr.Box(elem_classes="block"):
                        gr.Markdown("#### æ¨¡å‹æµ‹è¯•")
                        trained_models = gr.Dropdown(
                            label="é€‰æ‹©è®­ç»ƒå¥½çš„æ¨¡å‹",
                            choices=[],  # åˆå§‹ä¸ºç©ºåˆ—è¡¨
                            value=None
                        )
                        inference_question = gr.Textbox(
                            label="ä¸´åºŠé—®é¢˜", 
                            lines=3,
                            placeholder="è¯·è¾“å…¥ä¸€ä¸ªåŒ»ç–—ç›¸å…³çš„é—®é¢˜..."
                        )
                        inference_input = gr.Textbox(
                            label="è¡¥å……ä¿¡æ¯ï¼ˆå¯é€‰ï¼‰", 
                            lines=2,
                            placeholder="å¯ä»¥æä¾›é¢å¤–çš„ä¸´åºŠæ•°æ®å’Œç—…å²..."
                        )
                        with gr.Row():
                            inference_btn = gr.Button("è·å–å›ç­”", variant="primary", elem_classes="primary-btn")
                            refresh_models_btn = gr.Button("åˆ·æ–°æ¨¡å‹åˆ—è¡¨", variant="secondary")
                
                with gr.Column():
                    with gr.Box(elem_classes="block"):
                        gr.Markdown("#### AIå›å¤")
                        inference_output = gr.Textbox(label="ä¸´åºŠè§£ç­”", lines=10)
        
        with gr.Accordion("åŒ»ç–—æ•°æ®è®­ç»ƒ", open=True):
            with gr.Row():
                with gr.Column():
                    with gr.Box(elem_classes="block"):
                        gr.Markdown("#### æ•°æ®é€‰æ‹©")
                        qa_data_source = gr.Dropdown(
                            choices=[
                                "temp/recommendation_qa_data.json",
                                "temp/test_report_qa_data.json",
                                "ä½¿ç”¨åˆå¹¶æ•°æ®é›†"
                            ],
                            label="é€‰æ‹©ä¸´åºŠæ•°æ®æº",
                            value="ä½¿ç”¨åˆå¹¶æ•°æ®é›†"
                        )
                        with gr.Row():
                            qa_data_refresh_btn = gr.Button("åˆ·æ–°å¯ç”¨æ•°æ®é›†", variant="secondary")
                            qa_data_preview_btn = gr.Button("é¢„è§ˆæ•°æ®", variant="secondary")
                    
                with gr.Column():
                    qa_data_preview = gr.Textbox(label="æ•°æ®é¢„è§ˆ", lines=10)
                    
            gr.Markdown("---")
            gr.Markdown("### åŸºäºä¸´åºŠæ•°æ®è®­ç»ƒæ¨¡å‹")
            
            with gr.Row():
                with gr.Column():
                    with gr.Box(elem_classes="block"):
                        gr.Markdown("#### æ¨¡å‹é…ç½®")
                        qa_sft_model_name = gr.Dropdown(
                            choices=[
                                "unsloth/Qwen2.5-0.5B", 
                                "unsloth/Qwen2.5-1.5B", 
                                "unsloth/Qwen2.5-3B",
                                "unsloth/Qwen2.5-7B",
                                "unsloth/Qwen2.5-14B"
                            ],
                            label="é€‰æ‹©åŸºç¡€æ¨¡å‹",
                            value="unsloth/Qwen2.5-1.5B"
                        )
                        with gr.Row():
                            qa_training_epochs = gr.Slider(label="è®­ç»ƒè½®æ¬¡", minimum=1, maximum=10, value=3, step=1)
                            qa_learning_rate = gr.Slider(label="å­¦ä¹ ç‡", minimum=1e-5, maximum=5e-4, value=2e-4, step=1e-5)
                        qa_train_btn = gr.Button("å¼€å§‹è®­ç»ƒ", variant="primary", elem_classes="primary-btn")
                
                with gr.Column():
                    with gr.Box(elem_classes="block"):
                        gr.Markdown("#### è®­ç»ƒè¿›åº¦")
                        qa_training_status = gr.Textbox(label="è®­ç»ƒçŠ¶æ€", lines=5)
        
        # å­˜å‚¨Agentçš„çŠ¶æ€
        chat_agent_state = gr.State(value=None)
        
        # è®¾ç½®CoTæ•°æ®ç”Ÿæˆä¸è®­ç»ƒç›¸å…³çš„äº‹ä»¶
        create_agent_btn.click(
            fn=create_chat_agent,
            inputs=[openai_api_key, model_choice, system_message],
            outputs=[chat_agent_state, agent_status]
        )
        
        def load_qa_file(file):
            if file is None:
                return None
            with open(file.name, "r", encoding="utf-8") as f:
                return f.read()
        
        qa_data_file.change(
            fn=load_qa_file,
            inputs=qa_data_file,
            outputs=qa_data
        )
        
        generate_cot_btn.click(
            fn=generate_cot_data,
            inputs=[chat_agent_state, qa_data],
            outputs=[cot_output, alpaca_file_path]
        )
        
        upload_hf_btn.click(
            fn=upload_dataset_to_huggingface,
            inputs=[alpaca_file_path, hf_token, hf_username, hf_dataset_name],
            outputs=upload_status
        )
        
        train_btn.click(
            fn=train_model,
            inputs=[sft_model_name, alpaca_file_path, training_epochs, learning_rate],
            outputs=training_status
        )
        
        # æ›´æ–°æ¨¡å‹åˆ—è¡¨çš„å‡½æ•°
        def update_model_list():
            if os.path.exists("temp/trained_models"):
                return [d for d in os.listdir("temp/trained_models") if os.path.isdir(os.path.join("temp/trained_models", d))]
            return []
        
        # æ·»åŠ åˆ·æ–°æ¨¡å‹åˆ—è¡¨çš„äº‹ä»¶
        refresh_models_btn.click(
            fn=update_model_list,
            inputs=[],
            outputs=trained_models
        )
        
        # åœ¨è®­ç»ƒå®Œæˆåè‡ªåŠ¨åˆ·æ–°æ¨¡å‹åˆ—è¡¨
        train_btn.click(
            fn=update_model_list,
            inputs=[], 
            outputs=trained_models
        )
        
        inference_btn.click(
            fn=lambda model, q, i: inference_with_model(f"temp/trained_models/{model}", q, i) if model else "è¯·å…ˆé€‰æ‹©ä¸€ä¸ªæ¨¡å‹",
            inputs=[trained_models, inference_question, inference_input],
            outputs=inference_output
        )
        
        # Define function to get available QA datasets
        def get_available_qa_datasets():
            """è·å–å¯ç”¨çš„QAæ•°æ®é›†åˆ—è¡¨"""
            datasets = []
            
            # æ£€æŸ¥åŸºæœ¬æ•°æ®é›†
            basic_datasets = [
                "temp/recommendation_qa_data.json",
                "temp/test_report_qa_data.json"
            ]
            
            for dataset in basic_datasets:
                if os.path.exists(dataset):
                    datasets.append(dataset)
            
            # æ£€æŸ¥å¯¼å‡ºç›®å½•ä¸­çš„åˆå¹¶æ•°æ®é›†
            if os.path.exists("export"):
                for file in os.listdir("export"):
                    if file.startswith("merged_qa_data_") and file.endswith(".json"):
                        datasets.append(f"export/{file}")
            
            # æ·»åŠ ä¸€ä¸ªå›ºå®šé€‰é¡¹
            datasets.append("ä½¿ç”¨åˆå¹¶æ•°æ®é›†")
            
            return datasets
        
        # Now add event handlers for the new buttons at the end of the model training tab section
        qa_data_refresh_btn.click(
            fn=get_available_qa_datasets,
            inputs=None,
            outputs=qa_data_source
        )
        
        qa_data_preview_btn.click(
            fn=lambda source: load_qa_data(source) if source != "ä½¿ç”¨åˆå¹¶æ•°æ®é›†" else "è¯·é€‰æ‹©å…·ä½“çš„æ•°æ®é›†",
            inputs=qa_data_source,
            outputs=qa_data_preview
        )
        
        # Function to prepare data and start training
        def prepare_qa_data_and_train(data_source, model_name, epochs, learning_rate):
            """å‡†å¤‡QAæ•°æ®å¹¶å¼€å§‹è®­ç»ƒ"""
            try:
                # å¦‚æœé€‰æ‹©äº†"ä½¿ç”¨åˆå¹¶æ•°æ®é›†"ï¼Œåˆ™åˆ›å»ºä¸€ä¸ªæ–°çš„åˆå¹¶æ•°æ®é›†
                if data_source == "ä½¿ç”¨åˆå¹¶æ•°æ®é›†":
                    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
                    merged_path = f"export/merged_qa_data_{timestamp}.json"
                    
                    # åˆå¹¶å¯ç”¨çš„åŸºç¡€æ•°æ®é›†
                    basic_datasets = []
                    if os.path.exists("temp/recommendation_qa_data.json"):
                        basic_datasets.append("temp/recommendation_qa_data.json")
                    if os.path.exists("temp/test_report_qa_data.json"):
                        basic_datasets.append("temp/test_report_qa_data.json")
                    
                    if not basic_datasets:
                        return "æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„åŸºç¡€æ•°æ®é›†è¿›è¡Œåˆå¹¶"
                    
                    # æ‰§è¡Œåˆå¹¶
                    merge_result = merge_qa_datasets(basic_datasets, merged_path)
                    data_source = merged_path
                
                # è½¬æ¢ä¸ºè®­ç»ƒæ ¼å¼
                _, training_file = convert_qa_to_training_format(data_source)
                
                if not training_file:
                    return "æ•°æ®è½¬æ¢å¤±è´¥"
                
                # å¼€å§‹è®­ç»ƒ
                training_result = train_model(model_name, training_file, epochs, learning_rate)
                
                return f"å·²ä½¿ç”¨QAæ•°æ®å¼€å§‹è®­ç»ƒ\n{training_result}"
            except Exception as e:
                return f"å‡†å¤‡è®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}"
        
        # Add the training event handler
        qa_train_btn.click(
            fn=prepare_qa_data_and_train,
            inputs=[qa_data_source, qa_sft_model_name, qa_training_epochs, qa_learning_rate],
            outputs=qa_training_status
        )
    
    # Replace the existing "10. QAæ•°æ®ç®¡ç†" tab with a call to the function
    qa_data_components = create_qa_data_management_tab()
    
    gr.Markdown(
        """
        <div class="footer">
        Â© 2023-2024 åŒ»é™¢æ™ºèƒ½æ£€éªŒå¹³å° | ç”±å…ˆè¿›å¤šæ™ºèƒ½ä½“ç³»ç»Ÿæ¡†æ¶é©±åŠ¨ | æ”¯æŒåŒ»ç–—è¯Šæ–­ã€æ£€éªŒé¡¹ç›®æ¨èã€ç»“æœåˆ†æã€æŠ¥å‘Šç”ŸæˆåŠç§‘ç ”æ•°æ®ç®¡ç†
        <br><small>åŒ»ç–—å™¨æ¢°å¤‡æ¡ˆå·ï¼šXXXXX-XXXXXXX | æŠ€æœ¯æ”¯æŒï¼šåŒ»å­¦AIéƒ¨é—¨</small>
        </div>
        """
    )

# å¯åŠ¨åº”ç”¨
if __name__ == "__main__":
    app.launch(share=True) 